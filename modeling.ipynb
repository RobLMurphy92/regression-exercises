{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data for Modeling\n",
    "\n",
    "**Wrangle**\n",
    "\n",
    "1. Acquired data from student-mat.csv. \n",
    "\n",
    "2. Create dummy vars\n",
    "\n",
    "3. Split data \n",
    "\n",
    "4. Scale data\n",
    "\n",
    "wrangle.wrangle_student_math(path) returns the following: \n",
    "\n",
    "| Object Returned | Description | Purpose                          |\n",
    "|:-------------------|:--------------------------------|:----------------------------------------------------|\n",
    "| 1. df | **Dataframe**, **Feature** and **target** variables, **Unscaled**, Dummy vars **with** original categorical vars | New features, additional cleaning needed, etc. |\n",
    "| 2. X_train_exp | **Dataframe**, **Feature** variables only, **Unscaled**, Dummy vars **with** original categorical vars | Exploration & analysis     |\n",
    "| 3. X_train | **Dataframe**, **Feature** variables only, **Scaled**, Dummy vars **without** original categorical vars | Feature selection, fit models, make predictions |\n",
    "| 4. y_train | **Series**, **Target** variable only, **Unscaled** | Feature selection, evaluate model predictions |\n",
    "| 5. X_validate | **Dataframe**, **Features** variables only, **Scaled**, Dummy vars **without** original categorical vars | Make predictions using top models |\n",
    "| 6. y_validate | **Series**, **Target** variable only, **Unscaled** | Evaluate model predictions made from X_validate to assess overfitting | \n",
    "| 7. X_test | **Dataframe**, **Features** variables only, **Scaled**, Dummy vars **without** original categorical vars | Make predictions using best model|\n",
    "| 8. y_test | **Series**, **Target** variable only, **Unscaled** | Evaluate model predictions made from X_test to estimate future performance on new data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrangle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# modeling methods\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "path='https://gist.githubusercontent.com/ryanorsinger/55ccfd2f7820af169baea5aad3a9c60d/raw/da6c5a33307ed7ee207bd119d3361062a1d1c07e/student-mat.csv'\n",
    "\n",
    "df, \\\n",
    "X_train_exp, \\\n",
    "X_train, \\\n",
    "y_train, \\\n",
    "X_validate, \\\n",
    "y_validate, \\\n",
    "X_test, \\\n",
    "y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head of our X:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Target Variable/y**\n",
    "\n",
    "This helps us determine which type of algorithm we may want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how do we determine distribution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "### Baseline\n",
    "\n",
    "About the initial baseline: \n",
    "\n",
    "> Before we begin making models, we need to know how well we can estimate (predict) the final grade (G3) without using any features. This is often done by predicting every observation's target value to be the mean or the median. E.g. we could predict every student's final grade to be the mean final grade of all the students in our training sample. We will try both the mean and the median, see which performs best, and set that evaluation metric value as our baseline performance to beat. \n",
    "\n",
    "\n",
    "1. Predict all final grades to be 10.52, which is equal to the mean of G3 for the training sample. Store in `y_train['G3_pred_mean']`. \n",
    "\n",
    "2. Predict all final grades to be 11, which is equal to the median of G3 for the training sample. Store in `y_train['G3_pred_median']`.  \n",
    "\n",
    "3. Compute the RMSE comparing actual final grade (G3) to G3_pred_mean. \n",
    "\n",
    "4. Compute the RMSE comparing actual final grade (G3) to G3_pred_median. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need y_train and y_validate to be dataframes to append the new columns with predicted values. \n",
    "y_train = \n",
    "y_validate = \n",
    "\n",
    "# 1. Predict G3_pred_mean\n",
    "G3_pred_mean = \n",
    "y_train['G3_pred_mean'] = \n",
    "y_validate['G3_pred_mean'] = \n",
    "\n",
    "# 2. compute G3_pred_median\n",
    "G3_pred_median = \n",
    "y_train['G3_pred_median'] = \n",
    "y_validate['G3_pred_median'] = \n",
    "\n",
    "# 3. RMSE of G3_pred_mean\n",
    "rmse_train =\n",
    "rmse_validate = \n",
    "\n",
    "print(\"RMSE using Mean\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))\n",
    "\n",
    "# 4. RMSE of G3_pred_median\n",
    "rmse_train = mean_squared_error(y_train.G3, y_train.G3_pred_median) ** .5\n",
    "rmse_validate = mean_squared_error(y_validate.G3, y_validate.G3_pred_median) ** .5\n",
    "print(\"RMSE using Median\\nTrain/In-Sample: \", round(rmse_train, 2), \n",
    "      \"\\nValidate/Out-of-Sample: \", round(rmse_validate, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## \n",
    "# ~~~~~~~~~addendum~~~~~~~\n",
    "#  we will incrementally build \n",
    "# a dataframe for comparison of \n",
    "# our metrics for model selection\n",
    "######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to visualize actual vs predicted. \n",
    "plt.hist(####, color='blue', alpha=.5, label=\"Actual Final Grades\")\n",
    "plt.hist(####, bins=1, color='red', alpha=.5, rwidth=100, label=\"Predicted Final Grades - Mean\")\n",
    "plt.hist(####, bins=1, color='orange', alpha=.5, rwidth=100, label=\"Predicted Final Grades - Median\")\n",
    "plt.xlabel(\"Final Grade (G3)\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression (OLS)\n",
    "\n",
    "1. Fit the model using X_train_scaled and the labels from y_train. \n",
    "\n",
    "2. Predict final grade for students in training sample using our model (lm). \n",
    "\n",
    "3. Evaluate using RMSE\n",
    "\n",
    "4. Repeat predictions and evaluation for validation. \n",
    "\n",
    "5. Compare RMSE train vs. validation. Overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "# \n",
    "# make the thing\n",
    "# \n",
    "\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "# \n",
    "# fit the thing\n",
    "# \n",
    "\n",
    "\n",
    "# predict train\n",
    "# \n",
    "# use the thing!\n",
    "# \n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = mean_squared_error(y_train.G3, y_train.G3_pred_lm) ** (1/2)\n",
    "# predict validate\n",
    "y_validate['G3_pred_lm'] = lm.predict(X_validate)\n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = mean_squared_error(y_validate.G3, y_validate.G3_pred_lm) ** (1/2)\n",
    "\n",
    "print(\"RMSE for OLS using LinearRegression\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## \n",
    "# ~~~~~~~~~addendum~~~~~~~\n",
    "#  we will incrementally build \n",
    "# a dataframe for comparison of \n",
    "# our metrics for model selection\n",
    "######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LassoLars\n",
    "\n",
    "1. Fit the model using X_train_scaled and the labels from y_train. \n",
    "\n",
    "2. Predict final grade for students in training sample using our model (lars). \n",
    "\n",
    "3. Evaluate using RMSE\n",
    "\n",
    "4. Repeat predictions and evaluation for validation. \n",
    "\n",
    "5. Compare RMSE train vs. validation. Overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series!\n",
    "\n",
    "# predict train\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "# predict validate\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "print(\"RMSE for Lasso + Lars\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## \n",
    "# ~~~~~~~~~addendum~~~~~~~\n",
    "#  we will incrementally build \n",
    "# a dataframe for comparison of \n",
    "# our metrics for model selection\n",
    "######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TweedieRegressor (GLM)\n",
    "\n",
    "1. Fit the model using X_train_scaled and the labels from y_train. \n",
    "\n",
    "2. Predict final grade for students in training sample using our model (glm). \n",
    "\n",
    "3. Evaluate using RMSE\n",
    "\n",
    "4. Repeat predictions and evaluation for validation. \n",
    "\n",
    "5. Compare RMSE train vs. validation. Overfitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import TweedieRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "\n",
    "# predict train\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "# predict validate\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "print(\"RMSE for GLM using Tweedie, power=1 & alpha=0\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "Using sklearn.preprocessing.PolynommialFeatures() + sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create the new features, based on value indicated for degree for train, validate & test. \n",
    "\n",
    "2. Fit the Linear Regression model\n",
    "\n",
    "3. Predict using the transformed (squared or cubed, e.g.) features \n",
    "\n",
    "4. Evaluate using RMSE\n",
    "\n",
    "5. Repeat predictions and evaluation for validation.\n",
    "\n",
    "6. Compare RMSE train vs. validation. Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PolynomialFeatures**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the polynomial features to get a new set of features\n",
    "pf = \n",
    "\n",
    "# fit and transform X_train_scaled\n",
    "X_train_degree2 = \n",
    "\n",
    "# transform X_validate_scaled & X_test_scaled\n",
    "X_validate_degree2 = \n",
    "X_test_degree2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LinearRegression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model object\n",
    "lm2 =\n",
    "\n",
    "# fit the model to our training data. We must specify the column in y_train, \n",
    "# since we have converted it to a dataframe from a series! \n",
    "\n",
    "\n",
    "# predict train\n",
    "y_train['G3_pred_lm2'] = \n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_train = \n",
    "\n",
    "# predict validate\n",
    "y_validate['G3_pred_lm2'] = \n",
    "\n",
    "# evaluate: rmse\n",
    "rmse_validate = \n",
    "\n",
    "print(\"RMSE for Polynomial Model, degrees=2\\nTraining/In-Sample: \", rmse_train, \n",
    "      \"\\nValidation/Out-of-Sample: \", rmse_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## \n",
    "# ~~~~~~~~~addendum~~~~~~~\n",
    "#  we will incrementally build \n",
    "# a dataframe for comparison of \n",
    "# our metrics for model selection\n",
    "######\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "**Plotting Actual vs. Predicted Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_validate.head()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(####, ####, alpha=.5, color=\"gray\", label='_nolegend_')\n",
    "plt.annotate(\"Baseline: Predict Using Mean\", (16, 9.5))\n",
    "plt.plot(y_validate.G3, y_validate.G3, alpha=.5, color=\"blue\", label='_nolegend_')\n",
    "plt.annotate(\"The Ideal Line: Predicted = Actual\", (.5, 3.5), rotation=15.5)\n",
    "\n",
    "plt.scatter(#####, ##### \n",
    "            alpha=.5, color=\"red\", s=100, label=\"Model: LinearRegression\")\n",
    "plt.scatter(####, ####, \n",
    "            alpha=.5, color=\"yellow\", s=100, label=\"Model: TweedieRegressor\")\n",
    "plt.scatter(####, ####, \n",
    "            alpha=.5, color=\"green\", s=100, label=\"Model 2nd degree Polynomial\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Actual Final Grade\")\n",
    "plt.ylabel(\"Predicted Final Grade\")\n",
    "plt.title(\"Where are predictions more extreme? More modest?\")\n",
    "# plt.annotate(\"The polynomial model appears to overreact to noise\", (2.0, -10))\n",
    "# plt.annotate(\"The OLS model (LinearRegression)\\n appears to be most consistent\", (15.5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Residual Plots: Plotting the Errors in Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_validate.head()\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.axhline(label=\"No Error\")\n",
    "plt.scatter(####, #### , \n",
    "            alpha=.5, color=\"red\", s=100, label=\"Model: LinearRegression\")\n",
    "plt.scatter(####, ####, \n",
    "            alpha=.5, color=\"yellow\", s=100, label=\"Model: TweedieRegressor\")\n",
    "plt.scatter(####, ####, \n",
    "            alpha=.5, color=\"green\", s=100, label=\"Model 2nd degree Polynomial\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Actual Final Grade\")\n",
    "plt.ylabel(\"Residual/Error: Predicted Grade - Actual Grade\")\n",
    "plt.title(\"Do the size of errors change as the actual value changes?\")\n",
    "plt.annotate(\"The polynomial model appears to overreact to noise\", (2.0, -10))\n",
    "plt.annotate(\"The OLS model (LinearRegression)\\n appears to be most consistent\", (15.5, 3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histograms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to visualize actual vs predicted. \n",
    "plt.figure(figsize=(16,8))\n",
    "plt.hist(####, color='blue', alpha=.5, label=\"Actual Final Grades\")\n",
    "plt.hist(####, color='red', alpha=.5, label=\"Model: LinearRegression\")\n",
    "plt.hist(####, color='yellow', alpha=.5, label=\"Model: TweedieRegressor\")\n",
    "plt.hist(####, color='green', alpha=.5, label=\"Model 2nd degree Polynomial\")\n",
    "plt.xlabel(\"Final Grade (G3)\")\n",
    "plt.ylabel(\"Number of Students\")\n",
    "plt.title(\"Comparing the Distribution of Actual Grades to Distributions of Predicted Grades for the Top Models\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# addendum: Comparing models DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection & Out-of-Sample Evaluation**\n",
    "\n",
    "Model selected: lm (using LinearRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predict on test\n",
    "\n",
    "# evaluate: rmse\n",
    "\n",
    "print(\"RMSE for OLS Model using LinearRegression\\nOut-of-Sample Performance: \", rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
